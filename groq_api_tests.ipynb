{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>created</th>\n",
       "      <th>owned_by</th>\n",
       "      <th>active</th>\n",
       "      <th>context_window</th>\n",
       "      <th>public_apps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>whisper-large-v3</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama-3.2-90b-vision-preview</td>\n",
       "      <td>model</td>\n",
       "      <td>1727226914</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama-3.2-90b-text-preview</td>\n",
       "      <td>model</td>\n",
       "      <td>1727285716</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama3-groq-8b-8192-tool-use-preview</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Groq</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llava-v1.5-7b-4096-preview</td>\n",
       "      <td>model</td>\n",
       "      <td>1725402373</td>\n",
       "      <td>Other</td>\n",
       "      <td>True</td>\n",
       "      <td>4096</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id object     created owned_by  active  \\\n",
       "0                      whisper-large-v3  model  1693721698   OpenAI    True   \n",
       "1          llama-3.2-90b-vision-preview  model  1727226914     Meta    True   \n",
       "2            llama-3.2-90b-text-preview  model  1727285716     Meta    True   \n",
       "3  llama3-groq-8b-8192-tool-use-preview  model  1693721698     Groq    True   \n",
       "4            llava-v1.5-7b-4096-preview  model  1725402373    Other    True   \n",
       "\n",
       "   context_window public_apps  \n",
       "0             448        None  \n",
       "1            8192        None  \n",
       "2            8192        None  \n",
       "3            8192        None  \n",
       "4            4096        None  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Testing one Groq API\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "\n",
    "url = \"https://api.groq.com/openai/v1/models\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "pd.DataFrame(response.json()['data'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq()\n",
    "\n",
    "# Model for tool-use\n",
    "MODEL = 'llama-3.1-70b-versatile'\n",
    "\n",
    "def calculate(expression):\n",
    "    \"\"\"Evaluate a mathematical expression\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return json.dumps({\"result\": result})\n",
    "    except:\n",
    "        return json.dumps({\"error\": \"Invalid expression\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conversation(user_prompt):\n",
    "    # Initialize the conversation\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a calculator assistant. Use the calculate function to perform calculations.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Define the available functions/tools for our model to use\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate\",\n",
    "                \"description\": \"Evaluate a mathematical expression\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The mathematical expression to evaluate in python 3.11. You have access to numpy as np.\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Make the initial API call to Groq\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL, # LLM to use\n",
    "        messages=messages, # Conversation history\n",
    "        stream=False,\n",
    "        tools=tools, # Available tools (i.e. functions) for our LLM to use\n",
    "        tool_choice=\"auto\", # Let our LLM decide when to use tools\n",
    "        max_tokens=4096 # Maximum number of tokens to allow in our response\n",
    "    )\n",
    "\n",
    "    # Extract the response and any tool call responses\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    \n",
    "    if tool_calls:\n",
    "            # Define the available tools that can be called by the LLM\n",
    "            available_functions = {\n",
    "                \"calculate\": calculate,\n",
    "            }\n",
    "            # Add the LLM's response to the conversation\n",
    "            messages.append(response_message)\n",
    "\n",
    "            # Process each tool call\n",
    "            for tool_call in tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                function_to_call = available_functions[function_name]\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                # Call the tool and get the response\n",
    "                function_response = function_to_call(\n",
    "                    expression=function_args.get(\"expression\")\n",
    "                )\n",
    "                # Add the tool response to the conversation\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"tool_call_id\": tool_call.id, \n",
    "                        \"role\": \"tool\", # Indicates this message is from tool use\n",
    "                        \"name\": function_name,\n",
    "                        \"content\": function_response,\n",
    "                    }\n",
    "                )\n",
    "            # Make a second API call with the updated conversation\n",
    "            second_response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=messages\n",
    "            )\n",
    "            # Return the final response\n",
    "            return second_response.choices[0].message.content, messages\n",
    "    \n",
    "\n",
    "prompt = \"What is 27970.928461529482 squared?\"\n",
    "\n",
    "res, messages = run_conversation(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a calculator assistant. Use the calculate function to perform calculations.'},\n",
       " {'role': 'user', 'content': 'What is 27970.928461529482 squared?'},\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_fccy', function=Function(arguments='{\"expression\": \"27970.928461529482 ** 2\"}', name='calculate'), type='function')]),\n",
       " {'tool_call_id': 'call_fccy',\n",
       "  'role': 'tool',\n",
       "  'name': 'calculate',\n",
       "  'content': '{\"result\": 782372839.0}'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of 27970.928461529482 squared is 782,372,839.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
